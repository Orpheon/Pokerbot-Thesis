\begin{description}[style=sameline,leftmargin=3cm,font=\normalfont]
  \item[$s$] A state\\
  \item[$a$] An action\\
  \item[$S$] A trajectory, ie. a set of states that follow each other\\
  \item[$\pi$] A policy, ie. a function that selects an action given a state\\
  \item[$t$] A time-step, ie. a specific point in time during a trajectory\\
  \item[$t_0$] The first time-step of a trajectory\\
  \item[$t_{end}$] The final time-step of a trajectory\\
  \item[$r(s)$] The reward given for reaching state $s$\\
  \item[$R(S)$] The total reward (potentially discounted) of trajectory $S$\\
  \item[$\gamma$] The time-discount factor of rewards, ie. how much a reward one time-step in the future is worth (generally a number like $0.99$)\\
  \item[$Env(s, a)$] The next state generated by the environment if action $a$ was undertaken at state $s$\\
  \item[$Term_{\pi}(s, a)$] The final state reached if an agent follows policy $\pi$ after acting $a$ from state $s$\\
  \item[$V_{\pi}(s)$] The value, or total reward, of following policy $\pi$ starting from state $s$\\
  \item[$V^*(s)$] The value, or total reward, of following the optimal policy starting from state $s$\\
  \item[$Q^*(s, a)$] The value, or total reward, of following the optimal policy after acting $a$ from state $s$\\
  \item[$\epsilon$] The chance of acting randomly to allow exploration in Q-Learning\\
  \item[$\psi$] The polyak averaging constant\\
  \item[$P_{\pi, s}(a)$] The probability of policy $\pi$ selecting action $a$ in state $s$\\
  \item[$\alpha$] The Soft Actor-Critic temperature, the relative weight of the entropy bonus to the reward\\
  \item[$\beta$] The "skill chain length" of TrueSkill, skill rating difference representing a 76\% win likelihood\\
  \item[$\mu$] The mean of a player's TrueSkill rating distribution, which is also assumed to be the player's actual rating\\
  \item[$p(Selected)$] Probability of an agent being selected in a climbing division\\
\end{description}