@article {Libratus,
	author = {Brown, Noam and Sandholm, Tuomas},
	title = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
	volume = {359},
	number = {6374},
	pages = {418--424},
	year = {2018},
	doi = {10.1126/science.aao1733},
	publisher = {American Association for the Advancement of Science},
	abstract = {Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold{\textquoteright}em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent.Science, this issue p. 418No-limit Texas hold{\textquoteright}em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold{\textquoteright}em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/359/6374/418},
	eprint = {https://science.sciencemag.org/content/359/6374/418.full.pdf},
	journal = {Science}
}

@article {Pluribus,
	author = {Brown, Noam and Sandholm, Tuomas},
	title = {Superhuman AI for multiplayer poker},
	volume = {365},
	number = {6456},
	pages = {885--890},
	year = {2019},
	doi = {10.1126/science.aay2400},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold{\textquoteright}em poker. However, poker games usually include six players{\textemdash}a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold{\textquoteright}em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker.Science, this issue p. 885; see also p. 864In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold{\textquoteright}em poker, the most popular form of poker played by humans.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/365/6456/885},
	eprint = {https://science.sciencemag.org/content/365/6456/885.full.pdf},
	journal = {Science}
}

@misc{PokerStrategy,
    title={Identifying Playerś Strategies in No Limit Texas Holdém Poker through the Analysis of Individual Moves},
    author={Luís Filipe Teófilo and Luis Paulo Reis},
    year={2013},
    eprint={1301.5943},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}

@incollection{TrueSkill_original,
title = {TrueSkill\texttrademark : A Bayesian Skill Rating System},
author = {Ralf Herbrich and Minka, Tom and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {569--576},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf}
}

@misc{TrueSkill_blog, url={http://www.moserware.com/2010/03/computing-your-skill.html},
journal={Computing Your Skill},
author={Moser, Jeff},
year={2010},
month={Mar}}

@misc{SAC_main,
    title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
    author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
    year={2018},
    eprint={1801.01290},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

﻿@Article{AlphaStar,
author={Vinyals, Oriol
and Babuschkin, Igor
and Czarnecki, Wojciech M.
and Mathieu, Micha{\"e}l
and Dudzik, Andrew
and Chung, Junyoung
and Choi, David H.
and Powell, Richard
and Ewalds, Timo
and Georgiev, Petko
and Oh, Junhyuk
and Horgan, Dan
and Kroiss, Manuel
and Danihelka, Ivo
and Huang, Aja
and Sifre, Laurent
and Cai, Trevor
and Agapiou, John P.
and Jaderberg, Max
and Vezhnevets, Alexander S.
and Leblond, R{\'e}mi
and Pohlen, Tobias
and Dalibard, Valentin
and Budden, David
and Sulsky, Yury
and Molloy, James
and Paine, Tom L.
and Gulcehre, Caglar
and Wang, Ziyu
and Pfaff, Tobias
and Wu, Yuhuai
and Ring, Roman
and Yogatama, Dani
and W{\"u}nsch, Dario
and McKinney, Katrina
and Smith, Oliver
and Schaul, Tom
and Lillicrap, Timothy
and Kavukcuoglu, Koray
and Hassabis, Demis
and Apps, Chris
and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
url={https://doi.org/10.1038/s41586-019-1724-z}
}
