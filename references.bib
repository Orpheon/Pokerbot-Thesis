@article {Libratus,
	author = {Brown, Noam and Sandholm, Tuomas},
	title = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
	volume = {359},
	number = {6374},
	pages = {418--424},
	year = {2018},
	doi = {10.1126/science.aao1733},
	publisher = {American Association for the Advancement of Science},
	abstract = {Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold{\textquoteright}em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent.Science, this issue p. 418No-limit Texas hold{\textquoteright}em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold{\textquoteright}em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/359/6374/418},
	eprint = {https://science.sciencemag.org/content/359/6374/418.full.pdf},
	journal = {Science}
}

@article {Pluribus,
	author = {Brown, Noam and Sandholm, Tuomas},
	title = {Superhuman AI for multiplayer poker},
	volume = {365},
	number = {6456},
	pages = {885--890},
	year = {2019},
	doi = {10.1126/science.aay2400},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold{\textquoteright}em poker. However, poker games usually include six players{\textemdash}a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold{\textquoteright}em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker.Science, this issue p. 885; see also p. 864In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold{\textquoteright}em poker, the most popular form of poker played by humans.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/365/6456/885},
	eprint = {https://science.sciencemag.org/content/365/6456/885.full.pdf},
	journal = {Science}
}

@misc{PokerStrategy,
    title={Identifying Playerś Strategies in No Limit Texas Holdém Poker through the Analysis of Individual Moves},
    author={Luís Filipe Teófilo and Luis Paulo Reis},
    year={2013},
    eprint={1301.5943},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018},
    url={https://github.com/openai/spinningup}
}

@incollection{TrueSkill_original,
title = {TrueSkill\texttrademark : A Bayesian Skill Rating System},
author = {Ralf Herbrich and Minka, Tom and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {569--576},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf}
}

@misc{TrueSkill_blog, url={http://www.moserware.com/2010/03/computing-your-skill.html},
title={Computing Your Skill},
author={Moser, Jeff},
year={2010},
month={03}}

@misc{Treys,
url={https://github.com/ihendley/treys},
title={Treys - A pure Python poker hand evaluation library},
author={Will Drevo and Mark Saindon and Imran Hendley},
year={2013-2019}
}

@misc{Pypokerengine,
url={https://github.com/ishikota/PyPokerEngine},
title={PyPokerEngine - Poker engine for AI development in Python},
author={ishikota},
year={2016-2017}
}

@misc{Numpy,
  author =    {Travis Oliphant},
  title =     {{NumPy}: A guide to {NumPy}},
  year =      {2006--},
  howpublished = {USA: Trelgol Publishing},
  url = "http://www.numpy.org/",
  note = {[Online; accessed <today>]}
 }

@misc{TrueSkill_code,
url={https://trueskill.org/},
title={TrueSkill Python Library},
author={Heungsub Lee},
year={2018},
month={09},
day={07}
}

@misc{SAC_main,
    title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
    author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
    year={2018},
    eprint={1801.01290},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@Article{AlphaStar,
author={Vinyals, Oriol
and Babuschkin, Igor
and Czarnecki, Wojciech M.
and Mathieu, Micha{\"e}l
and Dudzik, Andrew
and Chung, Junyoung
and Choi, David H.
and Powell, Richard
and Ewalds, Timo
and Georgiev, Petko
and Oh, Junhyuk
and Horgan, Dan
and Kroiss, Manuel
and Danihelka, Ivo
and Huang, Aja
and Sifre, Laurent
and Cai, Trevor
and Agapiou, John P.
and Jaderberg, Max
and Vezhnevets, Alexander S.
and Leblond, R{\'e}mi
and Pohlen, Tobias
and Dalibard, Valentin
and Budden, David
and Sulsky, Yury
and Molloy, James
and Paine, Tom L.
and Gulcehre, Caglar
and Wang, Ziyu
and Pfaff, Tobias
and Wu, Yuhuai
and Ring, Roman
and Yogatama, Dani
and W{\"u}nsch, Dario
and McKinney, Katrina
and Smith, Oliver
and Schaul, Tom
and Lillicrap, Timothy
and Kavukcuoglu, Koray
and Hassabis, Demis
and Apps, Chris
and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={11},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
url={https://doi.org/10.1038/s41586-019-1724-z}
}

@Article{Qlearn_convergence,
author={Watkins, Christopher J. C. H.
and Dayan, Peter},
title={Q-learning},
journal={Machine Learning},
year={1992},
month={05},
day={01},
volume={8},
number={3},
pages={279-292},
abstract={Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
issn={1573-0565},
doi={10.1007/BF00992698},
url={https://doi.org/10.1007/BF00992698}
}

@book{Sutton,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA}
}

@Article{Alphago,
author={Silver, David
and Schrittwieser, Julian
and Simonyan, Karen
and Antonoglou, Ioannis
and Huang, Aja
and Guez, Arthur
and Hubert, Thomas
and Baker, Lucas
and Lai, Matthew
and Bolton, Adrian
and Chen, Yutian
and Lillicrap, Timothy
and Hui, Fan
and Sifre, Laurent
and van den Driessche, George
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go without human knowledge},
journal={Nature},
year={2017},
month={Oct},
day={01},
volume={550},
number={7676},
pages={354-359},
abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
issn={1476-4687},
doi={10.1038/nature24270},
url={https://doi.org/10.1038/nature24270}
}

@misc{GamesRL,
    title={A Survey of Deep Reinforcement Learning in Video Games},
    author={Kun Shao and Zhentao Tang and Yuanheng Zhu and Nannan Li and Dongbin Zhao},
    year={2019},
    eprint={1912.10944},
    archivePrefix={arXiv},
    primaryClass={cs.MA}
}

@misc{OpenAIFive,
    title={Dota 2 with Large Scale Deep Reinforcement Learning},
    author={OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique Pondé de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
    year={2019},
    eprint={1912.06680},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article {Deepstack,
	author = {Morav{\v c}{\'i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
	title = {DeepStack: Expert-level artificial intelligence in heads-up no-limit poker},
	volume = {356},
	number = {6337},
	pages = {508--513},
	year = {2017},
	doi = {10.1126/science.aam6960},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents{\textquoteright} cards. Morav{\v c}{\'i}k et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold{\textquoteright}em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry.Science, this issue p. 508Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold{\textquoteright}em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/356/6337/508},
	eprint = {https://science.sciencemag.org/content/356/6337/508.full.pdf},
	journal = {Science}
}

@article{KendallTau,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2282833},
 abstract = {Adapting the usual manual methods of computing Kendall's tau to automatic computation result in a running time of order N2. A method is described with running time of order N log N.},
 author = {William R. Knight},
 journal = {Journal of the American Statistical Association},
 number = {314},
 pages = {436--439},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {A Computer Method for Calculating Kendall's Tau with Ungrouped Data},
 volume = {61},
 year = {1966}
}

@article{Scipy,
       author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
         Travis E. and {Haberland}, Matt and {Reddy}, Tyler and
         {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
         and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
         St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
         {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
         R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
         {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
         Eric W. and {Vand erPlas}, Jake and {Laxalde}, Denis and
         {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
         {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
         and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
         {van Mulbregt}, Paul and {Contributors}, SciPy 1. 0},
        title = "{SciPy 1.0: Fundamental Algorithms for Scientific
                  Computing in Python}",
      journal = {Nature Methods},
      year = "2020",
      volume={17},
      pages={261--272},
      adsurl = {https://rdcu.be/b08Wh},
      doi = {https://doi.org/10.1038/s41592-019-0686-2},
}

@book{Elo,
  added-at = {2019-01-08T11:02:22.000+0100},
  address = {New York},
  author = {Elo, Arpad E.},
  biburl = {https://www.bibsonomy.org/bibtex/24148b608c40e521f795b8057a924b5d0/dschiffner},
  description = {Amazon.com: The Rating Of Chess Players, Past & Present (9780668047210): Arpad E Elo: Books},
  interhash = {d2e8d25f3d6bf5c5d387a8d81f15eb45},
  intrahash = {4148b608c40e521f795b8057a924b5d0},
  isbn = {0668047216 9780668047210},
  keywords = {cati},
  publisher = {Arco Pub.},
  refid = {4504131},
  timestamp = {2019-01-08T13:02:59.000+0100},
  title = {The Rating of Chessplayers, Past and Present},
  url = {http://www.amazon.com/Rating-Chess-Players-Past-Present/dp/0668047216},
  year = 1978
}

@book{BellmanEq,
    place={Oxford},
    edition={2},
    title={Optimization in economic theory},
    publisher={Oxford Univ. Press},
    author={Dixit, Avinash K},
    year={2009},
    pages={164}
}

@misc{HoldemHandvalues,
    author = "{Wikipedia contributors}",
    title = "Texas hold 'em Hand Values--- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    url = "https://en.wikipedia.org/w/index.php?title=Texas_hold_%27em&oldid=971612259#Hand_values",
    note = "[Online; accessed 9-August-2020]"
}

@misc{QlearningMnih,
    title={Playing Atari with Deep Reinforcement Learning},
    author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year={2013},
    eprint={1312.5602},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{PokerStatMethods1,
    author = {Darse Billings and Denis Papp and Jonathan Schaeffer and Duane Szafron},
    title = {Opponent Modeling in Poker},
    booktitle = {},
    year = {1998},
    pages = {493--499},
    publisher = {AAAI Press}
}

@misc{PokerStatMethods2,
    title={Bayes' Bluff: Opponent Modelling in Poker},
    author={Finnegan Southey and Michael P. Bowling and Bryce Larson and Carmelo Piccione and Neil Burch and Darse Billings and Chris Rayner},
    year={2012},
    eprint={1207.1411},
    archivePrefix={arXiv},
    primaryClass={cs.GT}
}

@MastersThesis{PokerTreeSearch,
    author     =     {A.A.J. van der Kleij},
    title     =     {Monte Carlo Tree Search and Opponent Modeling through Player Clustering in no-limit Texas Hold’em Poker},
    school     =     {University of Groningen},
    address     =     {the Netherlands},
    year     =     {2010},
}

@inproceedings{PokerPlayerEmulation,
author = {Teófilo, Luís and Reis, Luís},
year = {2011},
month = {06},
pages = {73-82},
title = {Building a No Limit Texas Hold’em Poker Agent Based on Game Logs Using Supervised Learning},
doi = {10.1007/978-3-642-21538-4_8}
}

@conference{PokerReinforcementLearning,
author={Annija Rupeneite},
title={Building Poker Agent Using Reinforcement Learning with Neural Networks },
booktitle={Doctoral Consortium - DCINCO, (ICINCO 2014)},
year={2014},
pages={22-29},
publisher={SciTePress},
organization={INSTICC},
doi={},
isbn={},
}