Poker is a game requiring both handling imperfect information and risk management, and variants with more than 2 players have not been studied deeply by reinforcement learning research. We train poker-playing artificial intelligence with a custom high-performance training environment by simulating large numbers of competing agents in a sports-like league, and investigate the performance and behavior of simple deep reinforcement learning architectures on different metrics. We find that Deep Q-Learning outperforms Soft Actor-Critic, and that agent architecture choices matter more for broad behavior than the population of agents trained against. We also investigate the reliability of Microsoft TrueSkill as a metric of agent performance, and find its reliability depends strongly on agent behavior.