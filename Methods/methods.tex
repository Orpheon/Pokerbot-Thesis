\chapter{Theoretical Methods}

\todo{find all eg. -> e.g. same for ie. -> i.e.}

\section{Reinforcement Learning}
\label{ReinforcementLearningChapter}
\subsection{Basic Concepts}
Reinforcement learning is a subfield of machine learning where an agent optimizes its actions in a sequential task to maximize some reward signal. It does not require a dataset nor labels, but instead an explorable simulation.

An agent is a black-box system which receives observations of the current state (usually in the form of a numeric vector), processes this according to an internal policy function (which can have many representations), and outputs selected actions (as a numeric vector).

In a discrete environment, these actions along with external influences lead to a new state. A sequence of states is called a trajectory. Some states are associated with a reward; a real value that represents how desirable it is for the agent to be in that state. The agent seeks to maximize overall reward over its trajectories.

Tasks are divided into episodic and continuous tasks \cite[chapters 3.3 \& 3.4]{Sutton}. An episodic task is a finite task with a well-defined end, e.g. playing a chess game. Rewards of episodic tasks are often only known at the end of a single episode, and an agent iterates through many such episodes to map out the state space. In contrast, a continuous task may run forever (e.g. an exploration robot), and rewards trickle in at any time.

A concept often used in continuous tasks is time discounting, where rewards in the far future are considered less important than rewards in the near future. Formally, let $r(s)$ denote the reward of particular state $s$, $t$ denote a particular time-step, $S$ a trajectory of states from time-steps $t_0$ to $t_{end}$, and $\gamma$ the time discount factor with $0 \leq \gamma \leq 1$ (commonly ~0.99). Then the total reward $R(S)$ can be defined as
\begin{equation}
    R(S) \coloneqq \sum_{t=t_0}^{t_{end}}{r\big(S(t)\big)\gamma^{t-t_0}}
\end{equation}
which the agent tries to maximize.

Another important concept in reinforcement learning is the tradeoff between exploration and exploitation \cite[chapter 2.1]{Sutton}. An agent must both act in the way it believes will lead to greatest reward, but also explore actions with unknown outcomes, and reattempt actions that previously led to bad outcomes. Different algorithms represent this tradeoff in different ways, but it is always present in some form.

\subsection{Q-Learning}
A policy $\pi$ describes some mapping of states to actions, and captures the strategy of an agent. It is useful to define a value function $V_{\pi}(s)$ of a state and policy that describes the total reward from following that policy starting from that state\footnote{Some authors define the value function as the total reward from following the optimal policy, and do not condition it on $\pi$. We keep it general here to reuse the same equations when looking at approximations of the optimal policy, and to keep notation consistent}.

Denoting the action recommended by policy $\pi$ given state $s$ by $\pi(s)$, and the resulting next state by $Env\big(s, \pi(s)\big)$, we can define a recursive equation the value function must satisfy:
\begin{equation}
\label{bellman_value}
    V_{\pi}(s) = r(s) + \gamma V_{\pi} \Big(Env\big(s, \pi(s)\big)\Big)
\end{equation}

(\ref{bellman_value}) and its variants is known as the Bellman equation \cite{BellmanEq}. Intuitively speaking, it states that the total reward following a policy from a specific state is the reward of that state, plus the discounted total reward from the next state after having acted. One can define a similar equation in probabilistic terms if the next state after acting isn't deterministic.

The value function describes total reward starting from a state following a policy. However, it is helpful to consider the first action separately, and construct a so-called Q-function describing total reward following a policy after an arbitrary action $a$. We define it as:
\begin{equation}
    Q_{\pi}(s, a) \coloneqq r(s) + \gamma V_{\pi}\big(Env(s, a)\big)
\end{equation}
It follows that:
\begin{equation}
\label{bellman_q}
    Q_{\pi}(s, a) = r(s) + \gamma Q_{\pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big)
\end{equation}

We additionally define $V^*(s)$ and $Q^*(s, a)$, respectively, as the value and Q-functions of the optimal policy.

The core concept behind (deep) Q-Learning is to construct a policy using a function approximator (e.g. a neural network), training this approximator as a supervised learning problem to satisfy (\ref{bellman_q}), and always select the action with the highest predicted reward \cite{QlearningMnih}.
Over time, the policy will converge to $Q^*(s, a)$.\cite{Qlearn_convergence}

To train an approximator based on itself recursively is highly unstable, so it is common to use a frozen copy of the Q-function on the right-hand side of the equation. This copy is updated less frequently, or updated with smoothing applied through polyak averaging ($0 \leq \psi \leq 1$):
\begin{equation}
    Q_{\pi}^{copy} \coloneqq \psi Q_{\pi} + (1 - \psi) Q_{\pi}^{copy}
\end{equation}

Though (\ref{bellman_q}) is the general equation for Q-Learning, one can simplify it by introducing certain restrictions. In particular, in episodic tasks like poker, rewards can be set to 0 for all non-terminal states, and time-discounting becomes no longer reasonable unless one wishes to punish longer games, so $\gamma$ is set to 1. Let $Term_{\pi}(s, a)$ indicate the terminal state of following policy $\pi$ starting from $Env(s, a)$:
\begin{equation}
\label{Qlearn_nobellman}
\begin{split}
Q_{\pi}(s, a) &= r(s) + \gamma Q_{\pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big)\\
&= \begin{cases}
r(s) &\mbox{if s is terminal} \\
Q_{\pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big) &\mbox{otherwise}
\end{cases}\\
&= r\big(Term(s, a)\big)
\end{split}
\end{equation}
which is no longer recursive, and thus easier to train.

Q-Learning, as presented so far, is maximally exploitative; it never acts in what it believes might are suboptimal paths to explore what they are actually like. In isolation, this means that Q-Learning is susceptible to honing immediately on a suboptimal policy and never improving. To remedy this, we add a certain level of noise to the actions, such that an agent selects a random action with probability $\epsilon$ \cite[chapters 2.2 \& 2.3]{Sutton}. The size of $\epsilon$ controls the exploration-exploitation trade off for Q-Learning, and $\epsilon$ is set to 0 in finished/non-training agents.

\todo{match-up and trade-off without -}

A significant limitation of Q-Learning in practice is that the Q-function must be evaluated for each action to compare expected total reward. This does not scale performance-wise to large action spaces, and cannot be applied on continuous action spaces.

\subsection{Soft Actor-Critic}

Soft Actor-Critic is a recent evolution of several deep reinforcement learning algorithms building off of Q-Learning. It can handle both discrete and continuous action spaces, it can handle the exploration/exploitation trade off in a principled way, and it boasts more stable convergence properties than many of its competitors \cite{SAC_main}.

Like Q-Learning, SAC trains a Q-function that attempts to predict the value of an action given a state and policy. Unlike Q-Learning, it also trains a policy network, that given a state outputs an action, and is trained to maximize the current Q-function. This removes the scaling issues that plagued Q-Learning, and allows continuous action spaces.

This bootstrapping of two neural networks on each other is often unstable, so SAC uses a few tricks to stabilize it. Two separate Q-functions are trained, each to their own polyak-averaged target as in Q-Learning. When the policy function is trained, both are evaluated, and the minimum is taken as optimization target. This prevents spurious optimism in the Q-functions misleading the policy network.

To understand SAC's approach to exploration, it is necessary to introduce the concept of policy entropy. The entropy of a policy is a measure of how difficult it is to predict what action it will select given a state. A deterministic policy that always returns the same action given the same state has entropy zero. The entropy of a stochastic policy in a specific state can be calculated by:

\todo{rethink arguments}
\begin{equation}
    H(\pi, s) \coloneqq \sum_{a \in \pi(s)}{-P_{\pi, s}(a) log\big(P_{\pi, s}(a)\big)}
\end{equation} where $P_{\pi, s}(a)$ is the probability of policy $\pi$ choosing action $a$ in state $s$. This can be understood as "The entropy of policy $\pi$ in state $s$ is the sum of $-log\big(P_{\pi, s}(a)\big)$ for every possible action, weighed by the probability of $\pi$ selecting that action".

SAC implements so-called entropy regularization. Not only does it attempt to maximize long-term reward, but it also attempts to maximize long-term entropy, as high entropy means highly random action selection and thus a high amount of exploration. It does this by adding an entropy bonus to the update equations for both the Q-functions and the policy function, and by making the policy function stochastic and returning a distribution to sample from, not simply a value.

To train the Q-functions, equation (\ref{bellman_q}) is used with the added entropy bonus:
\begin{equation}
    Q_{\pi}(s, a) = r(s) + \gamma \bigg(Q_{\pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big) + \alpha H\big(\pi(s)\big)\bigg)
\end{equation}
$\alpha$ is called the temperature and is a parameter to control the exploration-exploitation trade off, as it regulates how strongly weighed the entropy bonus should be.

The policy function does not return an arbitrary distribution. For simplicity, the neural network returns a normal distribution by outputting the mean and the standard deviation. To squash this in a finite range given by the action space limits, the normal distribution is squashed by a $tanh(x)$ function, leading to the overall policy distribution being a tanh-squashed normal distribution. This restriction allows computing the entropy in closed form as a function of the given standard deviation \cite[explained in detail in the code comments of the SpinningUp SAC implementation]{SpinningUp2018}, so the entropy bonus can be returned by the model alongside the sampled action.

\section{TrueSkill}

TrueSkill is a skill rating system developed by Microsoft in 2007 that attempts to measure relative skill of players in repeated games \cite{TrueSkill_original} \cite{TrueSkill_blog}.

\todo{reword section}
It supports flexible team setups by assuming team skill to be the sum of player skill, draws, and more than 2 teams. It also boasts a solid theoretical foundation. This makes it much more suitable for Texas Hold'em (6-player free-for-all) than ELO \cite{Elo} or other common skill ranking systems.

TrueSkill assumes each player has a numeric skill rating. The delta between two players' skill ratings represents the likelihood of one player winning. The scale involved is arbitrary and defined through the choice of a parameter $\beta$, where a player being $\beta$ points above another implies that player has an approximately 76\% chance\footnote{Specifically, $\Phi\left(\frac{1}{\sqrt{2}}\right)$, where $\Phi(x)$ is the cumulative density function of the standard normal distribution.} of winning if matched against each other.

A player's true skill rating is unknown. Instead, TrueSkill models each player with normal distribution of where it believes their skill rating should be. The variance is then a direct measure of how certain TrueSkill is of a player's skill. First-time players start with a wide distribution, and as they accrue games over time the system learns more information about their skill and their distribution narrows.

When players match up against each other, TrueSkill performs a bayesian update, interpreting their previous skill rating distribution as a prior and the game results as an observation, returning new posterior belief distributions for players' skill rating \cite{TrueSkill_original} \cite{TrueSkill_blog}. Upsets where a good player is beaten by a bad player cause larger updates, and the wider the prior distribution is, the larger the update.

For judging victories, TrueSkill only takes the player ranking during a match into account, whether they won first place, won second place, and so on. It does not take into account by how much they won, a narrow victory is equivalent to an overwhelming one, and the skill rankings only reflect how likely it is for a player to beat another, not by how much.

This has important consequences regarding poker, as a bad player occasionally wins against a better player \todo{find better formulation} thanks to lucky card, and these occasional victories would have great impact on the ratings due to it being an upset. To remedy this, we group hands into a round of 10'000 hands, tally the total winnings of the round, and consider the full round a single match in TrueSkill. The larger the round, the lower the probability that a bad player beats a good player out of luck, and thus the more stable TrueSkill becomes.