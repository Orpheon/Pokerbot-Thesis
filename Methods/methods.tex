\chapter{Theoretical Methods}

\section{Reinforcement Learning}
\label{ReinforcementLearningChapter}
\subsection{Basic Concepts}
Reinforcement learning is a subfield of machine learning where an agent optimizes its actions in a sequential task to maximize some reward signal. It does not usually require a dataset nor labels, but instead an explorable simulation.

An \textit{agent} is a black-box system which receives \textit{observations} of the current state, usually in the form of a numeric vector, processes this according to an internal \textit{policy} (which can have many representations) and outputs selected \textit{actions}, also as a numeric vector.

In a discrete environment, these actions along with external influences lead to a new state. A sequence of states is called a \textit{trajectory}. Some states are associated with a \textit{reward}, a real value that represents how desirable it is for the agent to be in that state. The agent seeks to maximize overall reward.

Tasks are divided into \textit{episodic} and \textit{continuous} tasks \cite[Chapters 3.3 \& 3.4]{Sutton}. An episodic task is a finite task with a well-defined end, for example playing a chess game. Rewards of episodic tasks are often only known at the end of a single \textit{episode}, and an agent iterates through many such episodes to map out the state space. In contrast, a continuous task may run forever, and rewards will trickle in over time. \todo{think about mentioning a problem can be both episodic and continuous, eg. pole}

An concept often used in continuous tasks is \textit{time discounting}, where rewards in the far future are considered less important than rewards in the near future. Formally, let $r(s)$ denote the reward of particular state $s$, $t$ denote a particular time-step, $S$ a trajectory of states from time-steps $t_0$ to $t_{end}$, and $\gamma$ the time discount factor with $0 \leq \gamma \leq 1$. Then the total return $R(S)$ can be defined as
\todo{Remove $*$ as multiplication symbol}
\begin{equation}
    R(S) = \sum_{t=t_0}^{t_{end}}{r\big(S(t)\big)\gamma^{t-t_0}}
\end{equation}
which the agent tries to maximize.

Another important concept in reinforcement learning is the tradeoff between \textit{exploration} and \textit{exploitation} \cite[Chapter 2.1]{Sutton}. An agent must both act in the way they believe will lead to greatest reward, but also must explore actions with unknown outcomes, and occasionally even reattempt actions that have led to bad outcomes to find out whether that was just bad luck or an actual consequence. Different algorithms represent this tradeoff in different ways, but it is always present in some form.

\subsection{Q-Learning}
A policy describes some mapping of states to actions, and captures the "strategy" of an agent. It is useful to define a \textit{value function} $V_{\pi}(s)$ of a state and policy that describes the total reward from following that policy starting from that state\footnote{Some authors define the value function as the total reward from following the \textit{optimal} policy, and do not condition it on $\pi$. We keep it general here to reuse the same equations when looking at approximations of the optimal policy.}.

Denoting the action recommended by policy $\pi$ given state $s$ by $\pi(s)$, and the resulting next state by $Env\big(s, \pi(s)\big)$, we can define a recursive equation the value function must satisfy:
\begin{equation}
\label{bellman_value}
    V_{\pi}(s) = r(s) + \gamma V_{\pi} \Big(Env\big(s, \pi(s)\big)\Big)
\end{equation}

(\ref{bellman_value}) and its variants is known as the Bellman Equation\todo{cite bellman}. Intuitively speaking, it states that the total reward following a policy from a specific state is the reward of that state, plus the discounted total reward from the next state after having acted. One can define a very similar equation in probabilistic terms if the next state after action isn't fixed.

This allows us to construct the Q-function of a state $s$ and action $a$, conditioned on policy $\pi$:
\begin{equation}
    Q_{\pi}(s, a) = r(s) + \gamma V_{\pi}\big(Env(s, a)\big)
\end{equation}
It follows that
\begin{equation}
\label{bellman_q}
    Q_{\pi}(s, a) = r(s) + \gamma Q_{pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big)
\end{equation}

We define $V^*(s)$ and $Q^*(s, a)$, respectively, as the value and Q-functions of the optimal policy.

The core concept behind Q-Learning is to construct a policy using a function approximator (for example, a neural network), training this approximator as a supervised learning problem to satisfy the Bellman equation, and always select the action with the highest predicted reward.
Over time, the policy will converge to $Q^*(s, a)$ \cite{Qlearn_convergence}.

To train an approximator based on itself recursively is highly unstable, and so it is common to use a frozen copy of the Q-function on the right-hand side of the equation which is updated less frequently. Alternatively, a copy of the Q-function is used that is updated with smoothing applied, for instance through polyak averaging ($0 \leq \alpha \leq 1$):
\todo{go through all the equations and replace assignment with $\leftarrow$}
\begin{equation}
    Q_{\pi}^{copy} = \alpha * Q_{\pi} + (1 - \alpha) * Q_{\pi}^{copy}
\end{equation}

Though (\ref{bellman_q}) is the general equation for Q-Learning, one can simplify it by introducing certain restrictions. In particular, in episodic tasks like poker, rewards can be set to 0 for all non-terminal states, and time-discounting becomes no longer reasonable unless one wishes to punish longer games, so $\gamma$ is set to 1. Let $Term_{\pi}(s, a)$ indicate the terminal state of following policy $\pi$ starting from $Env(s, a)$:
\begin{equation}
\label{Qlearn_nobellman}
\begin{split}
Q_{\pi}(s, a) &= r(s) + \gamma Q_{\pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big)\\
&= \begin{cases}
r(s) &\mbox{if s is terminal} \\
Q_{\pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big) &\mbox{otherwise}
\end{cases}\\
&= r\big(Term(s, a)\big)
\end{split}
\end{equation}
which is no longer recursive, and thus easier to train.

Q-Learning, as presented so far, is maximally exploitative; it never acts in what it believes might be suboptimal paths to explore what they are actually like. In isolation, this means that Q-Learning is highly susceptible to honing immediately on a suboptimal policy and never improving. To remedy this, it is useful to add a certain level of noise to the actions, such that an agent selects a random action with probability $\epsilon$ \cite[Chapters 2.2 \& 2.3]{Sutton}. The size of $\epsilon$ controls the exploration-exploitation trade-off for Q-Learning, and of course $\epsilon$ is usually set to 0 in finished/non-training agents.

A significant limitation of Q-Learning in practice is that the Q-function must be evaluated on each action to compare expected return. This scales very badly performance-wise to large action spaces, and cannot be applied without modification on continuous action spaces.

\todo{cite minh 2013 somewhere}


\subsection{Soft Actor-Critic}

Soft Actor-Critic is a recent evolution of several deep reinforcement learning algorithms building off of Q-Learning. It can handle both discrete and continuous action spaces, it handles the exploration/exploitation trade-off in a principled way, and it boasts more stable convergence properties than many of its competitors \cite{SAC_main}.

Like Q-Learning, SAC trains a Q-function that attempts to predict the value of an action given a state and policy. Unlike Q-Learning, it also trains a policy network, that given a state outputs an action, and is trained to maximize the current Q-function. This removes the scaling issues that plagued Q-Learning, and allows continuous action spaces.

This bootstrapping of two neural networks on each other is often unstable, so SAC uses a few tricks to stabilize it. Two separate Q-functions are trained, each to their own polyak-averaged target as in Q-Learning. When the policy function is trained, both are evaluated, and the minimum is taken as optimization target. This helps prevent spurious optimism in the Q-functions completely misleading the policy network.

To understand SAC's approach to exploration, it is helpful to introduce the concept of policy entropy. The entropy of a policy is a measure of how difficult it is to predict what action will be taken under a specific policy given the state. A deterministic policy that always returns the same action given the same state has entropy zero. The entropy of a stochastic policy in a specific state can be calculated by:
\begin{equation}
    H\big(\pi(s)\big) = \sum_{a \in \pi(s)}{-P_{\pi, s}(a) log\big(P_{\pi, s}(a)\big)}
\end{equation} where $P_{\pi, s}(a)$ is the probability of choosing action $a$ under policy $\pi$ in state $s$. This can be understood as "The entropy of policy $\pi$ in state $s$ is the sum of $-log\big(P_{\pi, s}(a)\big)$ for every possible action, weighed by the probability of $\pi$ selecting that action".

SAC implements so-called entropy regularization. Not only does it attempt to maximize long-term reward, but it also attempts to maximize long-term entropy, as high entropy means very random action selection and thus a high amount of exploration. It does this by adding an entropy bonus to the update equations for both the Q-functions and the policy function, and by making the policy function return a distribution to sample from, not simply a value.

To train the Q-functions, the Bellman equation (\ref{bellman_q}) is used, with the added entropy bonus:
\begin{equation}
    Q_{\pi}(s, a) = r(s) + \gamma \bigg(Q_{pi}\Big(Env(s, a), \pi\big(Env(s, a)\big)\Big) + \alpha H\big(\pi(s)\big)\bigg)
\end{equation}
$\alpha$ is called the temperature, and is a parameter to control the exploration-exploitation trade-off, as it regulates how strongly weighed the entropy bonus should be.

The policy function does not return an arbitrary distribution, for simplicity the neural network usually returns a normal distribution, by outputting the mean and the standard deviation. To squash this in a finite range given by the action space limits, the normal distribution is squashed by a $tanh(x)$ function, leading to the overall policy distribution being a squashed normal distribution. This restriction allows computing the entropy in closed form as a function of the given standard deviation \cite[explained in detail in the code comments of the SpinningUp SAC implementation]{SpinningUp2018}, so the entropy bonus can be returned by the model alongside the sampled action.

\section{TrueSkill}

TrueSkill is a skill rating system developed by Microsoft in 2007 that attempts to measure relative skill of players in repeated games \cite{TrueSkill_original} \cite{TrueSkill_blog}.

It supports flexible team setups by assuming team skill to be the sum of player skill, draws, and more than 2 teams. It also boasts a solid theoretical foundation. This makes it much more suitable for texas hold'em (6-player free-for-all) than ELO \todo{cite ELO} or other common skill ranking systems.

TrueSkill assumes each player has a skill rating, which it represents as a real number. The delta between two players' skill represents the likelihood of one player winning. The scale involved is arbitrary, and defined through the choice of a parameter $\beta$, where a player being $\beta$ points above another implies that player has an approximately 76\% chance\footnote{Specifically, $\Phi\left(\frac{1}{\sqrt{2}}\right)$, where $\Phi(x)$ is the cumulative density function of the standard normal distribution.} of winning if matched together.

A player's actual skill rating is unknown. Instead, TrueSkill models each player with normal distribution of where it believes their skill rating is. The variance is then a direct measure of how certain TrueSkill is of a certain player's skill. First-time players start with a very wide distribution, and as they accrue games over time the system learns more information about their skill, and their distribution narrows.

When players match up against each other, in any configuration, TrueSkill performs a bayesian update, interpreting their previous skill distribution as a prior and the game results as an observation, returning new posterior belief distributions for players' skill. "Upsets" where a good player is beaten by a bad player cause larger updates, and the wider the prior distribution is the larger the updates as well. How the update is performed in detail is complicated and goes beyond the scope of this thesis, please refer to \cite{TrueSkill_original} and \cite{TrueSkill_blog}.

For judging victories, TrueSkill only takes the player ranking during a match into account, whether they won first place, won second place, and so on. It does not take into account how hard they won, a narrow victory is equivalent to a one-sided one, and the skill rankings only reflect how \textit{likely} it is for a player to beat another, not by how much.

This has important consequences regarding poker, as a bad player may easily occasionally win against a better player, and these occasional victories would have great impact on the ratings due to it being an upset. To remedy this, one can group hands into a round of several thousand hands, tally the total winnings of the round, and consider the full round a single match for the purposes of TrueSkill. The larger the round, the lower the probability that a bad player beats a good player out of luck, and thus the less unstable TrueSkill becomes.