\chapter{Introduction}
\section{Poker Competition}
The institute for Data Science of FHNW\footnote{University of Applied Sciences and Arts Northwestern Switzerland} ran a poker AI competition during the spring semester of 2020.  As one of the three teams our goal was to write a poker-playing bot in a specified framework which would be evaluated twice a week against those of the other teams at 6-player No-Limit Texas Hold'em Poker. This dissertation describes our approach, methods, internal evaluations, and insights during this competition.

\section{Approaches}
There exist many previous works on autonomously playing poker. Some authors model opponents statistically by assuming optimal play and inferring game state from opponent actions (e.g. \cite{PokerStatMethods1} for a frequentist approach or \cite{PokerStatMethods2} for a Bayesian approach), and others emulate players using supervised learning on databases of past games (e.g. \cite{PokerPlayerEmulation}). However, the most successful approaches are based on counter-factual regret (e.g. DeepStack \cite{Deepstack}, Libratus \cite{Libratus}, or Pluribus \cite{Pluribus}). Pluribus is unique in that it is the first implementation (to our knowledge) that achieved superhuman performance specifically in 6-player Texas Hold'em, instead of only 2-player games.

Deep reinforcement learning has emerged as one of the strongest approaches for game AI in general, reaching superhuman performance in Go \cite{Alphago}, Starcraft 2 \cite{AlphaStar}, Dota 2 \cite{OpenAIFive}, and a number of other games \cite{GamesRL}. We chose to apply deep reinforcement learning to poker, inspired by AlphaStar's approach of emulating a league of hundreds of competing agents \cite{AlphaStar}.