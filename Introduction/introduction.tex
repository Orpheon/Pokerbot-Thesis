\chapter{Introduction}
\section{Poker Competition}
A poker AI competition between three teams was run during the spring semester of 2020 by the Institute for Data Science of the FHNW\footnote{University of Applied Sciences and Arts Northwestern Switzerland}. As one of the three teams our goal was to write a bot in a specified framework, which would be evaluated twice a week against those of the other teams at 6-player No-Limit Texas Hold'em poker. This dissertation describes our approach, methods, internal evaluations, and insights during this competition.

\section{Poker}
No-Limit Texas Hold'em Poker is a turn-based game, where each player starts out with a fixed amount of cash they can bet in each round (called a hand in this thesis), and the winner of the hand collects all bets. The total winnings and loss of each player are summed over n hands, and whichever player ends up with the highest total wins.

\subsection{Cards}
The game is played is played with a standard 52-card french deck without jokers. At the start of the game each player receives two cards called hole cards that should not be revealed to the other players. Over the course of the game five more cards are revealed to all players, called community cards. If there is a contest, the winner is determined by hand strength, which depends on patterns of cards between all the cards a player can see (hole and community cards) \cite{HoldemHandvalues}.

\subsection{Positions}
Before the players are dealt their cards, one player receives the dealer button. This sets the betting order. Two other important roles are the small and big blind, who are forced to place an initial bet before the game starts. The seating positions are as follows:
\begin{itemize}
    \item Dealer Button (BTN)
    \item Small Blind (SB)
    \item Big Blind (BB)
    \item Early Position (EP), or also called Under the Gun (UTG) because this player is the first to act during the first phase.
    \item Middle Position (MP)
    \item Cut Off (CO)
\end{itemize}

\subsection{Actions}
A player can take one of three actions: fold, call or raise.

Folding means that the player drops out of the hand, loses their current investment, is not eligible for winning the pot, cannot take any actions until the next hand, but does not have to reveal their hole cards.

Calling is when a player matches the current highest bet, also called a check if no other player has raised during this phase.

Raising is when a player increases the current highest bet by raising their bet over the previous highest bet. There is a minimum amount of money that the player can raise by, to prevent long games where one player after another raises by a minuscule amount. A raise at the start of a betting round is also called a bet.

\subsection{Phases}
Each hand is divided into five phases. During the first four, all players that have not folded have to either call, raise or fold until either everyone but one player has folded, all remaining players check, or every player after the last raise has either called or folded.

\begin{itemize}
  \item Preflop (Each player receives their hole cards, the EP starts with betting)
  \item Flop (The first three community cards are revealed, the SB starts with betting)
  \item Turn (The fourth community card is revealed, the SB starts with betting)
  \item River (The fifth and last community card is revealed, the SB starts with betting)
  \item Showdown (This phase starts when either the river phase finishes or when a player goes all-in and not all other players fold. During this phase all players that have not folded yet reveal their cards and the player with the strongest hand receives the pot. If multiple players have equal hand strengths, the pot is divided equally between them.)
\end{itemize}

\subsection{Competition Rules}
The competition specified the rules have been specified more in depth, so that every team is able to train their agents accordingly. The python framework used is called PyPokerEngine \cite{Pypokerengine} and has been modified for the challenge. There is no guarantee that hands are played in order, because some parts might be parallelized to speed up evaluation. The SB is set to 1 and the BB to 2, each player starts every hand with 200. Each team can submit their agent twice per week, which is then directly evaluated against some very simple agents called baselines. This baseline evaluation is run over 10'000 hands and the team receives the result as soon as the evaluation is completed. If the evaluation fails for any reason (software bugs, timeouts, etc.) it does not count against the number of weekly evaluations and the last successful submission remains the current entry. Twice every week the current agents of the competing teams are pitted against each other and the three agents submitted by the supervisor. They are evaluated over 100'000 hands and the results are visible on a leaderboard to which all teams have access.

\todo{check correct capitalization of no-limit Texas Hold'em Poker}

\section{Possible Approaches}
There are many different approaches to autonomously playing poker; statistical methods \cite{PokerStatMethods1} \cite{PokerStatMethods2}, search tree traversal \cite{PokerTreeSearch} \cite{Deepstack}, player emulation \cite{PokerPlayerEmulation}, reinforcement learning \cite{Deepstack} \cite{Libratus} \cite{Pluribus} \cite{PokerReinforcementLearning}, etc.

We chose to focus on deep reinforcement learning with self-play because it is a relatively new approach with great results in other games \cite{GamesRL} \cite{OpenAIFive} \cite{Alphago} \cite{AlphaStar} and it does not require a training set. We were inspired by AlphaStar's \cite{AlphaStar} approach to training agents at a complex strategy game by emulating an entire league of players, and letting hundreds of mutations of agents match against each other.

Deep reinforcement learning has already found human and super-human success in 2-player poker \cite{Deepstack} \cite{Libratus}, and even 6-player No-Limit Texas Hold'em \cite{Pluribus}. However, all of these results use hybrid approaches, using neural networks only when the search space becomes too large for traversal. They also all use counterfactual regret minimization, which can be complex to simulate on a large scale. Our work uses comparatively simple algorithms on low-end hardware.
