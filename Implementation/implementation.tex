\chapter{Implementation}

\section{Divisions}
- Motivation behind split divisions
- all types of divisions and tying matchup to divisions


\section{Performance Metrics}

\subsection{Winnings}

\subsection{TrueSkill}


\section{Strategy Analysis}

We used several metrics to try to figure out how different agents behaved, and how they differed in their behavior. Some of these were primarily for debugging, others for insight-generation.

\subsection{Cashflow}

Some agents play slightly worse than others in all situations. Others might usually win, but lose very hard against certain exploiters. Yet others would prefer to "avoid losing", and make small gains but never really exploit others.

To distinguish which agents were exploiting which how much, we attempted to measure cashflow, how much money was each agent giving over to each agent on average.

\todo{IMAGE: insert example cashflow}

Cashflow was computed individually per round, though conceptually it would not be impossible to compute it on average throughout. However, it requires extensive logs containing the individual results of every single hand, to track who wins what, which slows training considerably.

\subsection{Strategy Vector}

In order to compare two different player strategies, one concept is to consider the strategies as black boxes and compare how they behave in a number of situations. This was the approach we chose for our so-called strategy vector.

Each agent to be analyzed was instantiated and fed fake game states, generated by iterating over number of visible cards, seating position, number of players who haven't folded yet, and investment into the pot (which also controlled pot size). For every combination of these, 10000 hands were generated, sorted into equally-sized card rank bins, and the agent's behavior was logged for every combination until sufficient samples were gathered for each bin.

The action space was divided into 53 bins, corresponding to folding, calling, checking, and raising in intervals of 4 chips (to a maximum of 200). Each of these bins contained the number of hands of a particular situation the agent chose to perform that action in.

The resulting \todo{check this number} 2880-dimensional list of situations with action frequencies in each was considered a characterization of that agent's playstyle, called that agent's strategy vector, and saved.

\todo{say something here about strat vector being useful both for quickly changing queries without recomputing all of the games, and also hopes for multidimensional clustering approaches that didn't work out all that well.}

Card ranks were computed differently based on the number of cards. For pre-flop, we used monte-carlo simulation until stability to generate a table of each possible pair, which let us rank them from best to worse, and divide these into deciles.

For flop, turns, and rivers, this is not feasible. Instead, we computed the card rank as evaluated by Treys \cite{Treys} directly, and compared that with a precomputed table of the number of hands with that number of cards that were better. This is not perfectly equal to the likelihood of a hand winning, but much easier to compute, so we used this approach as an approximation. \todo{consider giving an example, here}

\todo{IMAGE: card rank distributions: flop, turn, river}

\subsection{Aggression and Tightness}

Two common metrics for playstyle in poker are aggression and tightness \cite{PokerStrategy}.

Aggression compares the number of situations where a player increases the pot, vs just calling, and as such measures a player's proactivity. It can be defined as:
\begin{equation}
    Aggression = \frac{\# bets + \# raises}{\# calls}
\end{equation} where \# indicates the number of each betting action. Note that checks are not included in the calls.

Because this ratio can go all the way to infinity, it is somewhat difficult to plot, and the differences between agents are overstated at very small numbers of calls. To remedy this, we use a distortion of aggression in all our plots:
\begin{equation}
\begin{split}
Distorted\ Aggression &= \begin{cases}
\frac{Aggression}{Aggression + 1} &\mbox{if }Aggression\mbox{ is finite} \\
1 &\mbox{otherwise}
\end{cases}
\end{split}
\end{equation}
This compresses aggression to a nice $[0, 1]$ range, with $\frac{1}{2}$ being the natural midpoint of the same number of raises/bets and calls.

Tightness is defined by how many hands a player folds. It is defined simply as:
\begin{equation}
    Tightness = \frac{\# folds}{\# calls + \# bets + \# raises}
\end{equation}
We decided to ignore checks, as most of our agents have a built-in behavior where they always check if they can and called or folded. This makes the number of checks mostly dependent on the situation numbers, and not on the agent behavior.

Both metrics were computed based off of the precomputed strategy vectors, as this both gave a consistent set of situations between agents (unlike game logs), and sufficient information to rapidly compute them.
\section{Agents}

\subsection{Baselines}



\subsection{Q-Learning}
-Definition of Q-function
-Why discrete action space is forced, and how we discretized our behavior
-Bellman equation, why we both use and ignore it
-variants we used in our engine
(\cite{SpinningUp2018})

\subsection{Soft Actor-Critic}
\cite{SAC_main}
-Basic ideas behind SAC:
    -policy network + value network
    -train value network on outcomes, train policy network on value network
    -policy outputs a gaussian distribution, is rewarded for a wide spread
    -this encourages exploration where it doesn't massively reduce value
-Why: it's the latest in DeepRL
-Parameters
(\cite{SpinningUp2018})


\section{Vectorized Poker Engine}
-Requirements from competition
-Need for a fast engine that allowed batching
    -hence, own engine
-pypokerengine too slow
-Treys (\cite{Treys})