\chapter{Implementation}

\section{Training Environment}
We set up a modular training environment to compare different agent populations. The different divisions can be used to observe how agents can influence each other, which match-up method produces the best results, and neatly separates the training from the testing set. The following sections will go into further detail elaborating on the different modules and their interaction.

\subsection{Agent Manager}
The agent manager is the single source of truth for all agents, ans is responsible for instantiating all agents. The agent manager tags every agent with a unique id, so all other components only need to store the agent id, and and not all the information. This removes redundancy and also makes it easy to change the agent information without side effects. The agent manager stores all important information about an agent: the type of the agent, the paths to the model files, whether the agent is trainable or not, the origin division and its unique name.
\todo{explain more}

\subsection{Leaderboard}
A leaderboard ranks agents from best to worst according to one of the metrics. There are several different types of leaderboards using different metrics (e.g. trueskill, winnings, etc.). When a match in a division concludes, all leaderboards attached to the division are updated with the results. Further detail on the different metrics in section \ref{PerformanceMetricsSection}.

\subsection{Divisions}
\todo{explain what a division is}
A division is responsible for generating match-ups between agents, running the games and updating the leaderboards with the results. There are several types of divisions, all with different match-up rules.
Divisions differentiate between two types of agents: teachers and students. A student is trainable, using one of the algorithms discussed in \ref{ReinforcementLearningChapter} and adapts its behavior during play. A teacher agent does not change its behavior and is either a baseline that has a hard-coded behavior or a clone of a student agent with fixed weights.
Divisions usually have their own set of agents and do not influence agents outside of this, there are some exceptions though as will be further explained.

\subsubsection{Random Division}
The random division requires at least one student and one teacher agent and keeps a list of its own agents. The matchups are generated by randomly selecting one student teacher and five teacher agents. These teacher agents do not have to be distinct and it is possible that the same agent is chosen five times.

\subsubsection{Climbing Division}
The climbing division also requires at least one student and one teacher agent and keeps a list of its own agents. Unlike the random division, this division requires a trueskill based leaderboard to function. The goal of this division is to optimize the student agents against better teachers instead of all agents. This division selects the teacher agents by using their trueskill rating. The probability of an agent being selected is.
\begin{equation}
  P_i = \frac{{ts_i}^2}{\sum_{j=0}^{n} {ts_j}^2}
\end{equation}
This is repeated five times, hence it is very likely that the best agent is chosen multiple times.

But this has two problems:
First, a new agent is assumed to have an average rating, but is very likely to be the best or one of the best agents. Because of this the agent might only very rarely play and therefore take a long time to converge to its actual trueskill rating. Second, with a large number of agents there is an inequality between the number of games played which also causes the trueskill value of the rarely appearing agents to be unreliable.
To counteract this problem, there is a fixed probability that instead of the regular match-up with one student and five teachers, the matchup is generated with six teachers. This has the sole purpose of improving the reliability of the trueskill ratings and therefore improving the regular match-up. For this six teacher match-up a random agent is selected and five other teachers with similar ranking are chosen as opponents, if enough agents are available, no agent is chosen multiple times as this maximises the number of agent rankings being updated.

\subsubsection{Permanent Evaluation Similar Division}
Short PermEvalSimilar, this division is intended to evaluate all agents, not just agents inside the division. The goal of this division is to evaluate agents intradivision to not only test how agents react to previously unknown opponents but also to evaluate which divisions produce better agents.
This division does not have an internal agent list, but uses the agent manager directly. The division looks at all available agents, except for student agents, to not influence the learning process. The Division selects one agent at random and chooses five more agents with similar rank, these five agents are unique if enough are available.

\subsubsection{Permanent Evaluation Direct Division}
\todo{changed name, fix content}
Short PermaEvalDirect, this division is special, as it compares only two agents against each other. It does this by selecting two random agents from all agents and creates three instances of each agent. The two agents cannot be the same one and the division tracks already run match-ups and prioritizes those that were not run before. This division is specifically designed to be used for the winnings \todo{refere to winning metric} metric.

\subsection{Division Manager}
The division manager works very similar to the agent manager that it handles all divisions with their configurations and tags every division with a unique id. This is used to simplify the running of games, as the user can simply add divisions with the desired configuration and agent constellation and then specify which divisions should be run.

\section{Performance Metrics}\label{PerformanceMetricsSection}

\subsection{TrueSkill}
We did not reimplement TrueSkill, instead using an existing python implementation \cite{TrueSkill_code}. We used a $\beta$ of $\frac{25}{6}$ and a starting distribution $\mathcal{N}(\mu=100, \sigma=30)$ for new agents.

\subsection{Winnings}
The winnings metric compares agents in a direct confrontation against each other, for this both agents are matched-up with three copies of both agents. The seats are randomized to minimize the advantage gained by a better seat. Ideally all seating combinations would be run over an equal amount of hands, but this was not implemented because of the increased computational time needed. For both agents the average winning is calculated and added to a running average matrix of all agent types. \todo{add graphic here or only at results?} To improve runtime, match-ups that have not been played are first in line for the next evaluation. Because of the quadratic growth of match-ups needed this metric might not be a good choice for a very large number of agents.
To generate a leaderboard from the winning matrix, the following measures might be used for different purposes:
- mean of winnings
- median of winnings, this has the advantage of not being screwed by different performances against very bad agents (e.g. the baseline agents)
- percentiles (e.g. 10pctl) of winnings, this has the advantage of showing how the agent performs against it's worst match-ups, not confused with the best agents, because of possible exploiter agents

\todo{fix numbering}

\section{Strategy Analysis}

We used several metrics to try to figure out how different agents behaved, and how they differed in their behavior. Some of these were primarily for debugging, others for insight-generation.

\subsection{Cashflow}

Some agents play slightly worse than others in all situations. Others might usually win, but lose very hard against certain exploiters. Yet others would prefer to "avoid losing", and make small gains but never really exploit others.

To distinguish which agents were exploiting which how much, we attempted to measure cashflow, how much money was each agent giving over to each agent on average.

\todo{IMAGE: insert example cashflow}

Cashflow was computed individually per round, though conceptually it would not be impossible to compute it on average throughout. However, it requires extensive logs containing the individual results of every single hand, to track who wins what, which slows training considerably.

\subsection{Strategy Vector}
\label{StrategyVectorComputation}
In order to compare two different player strategies, one concept is to consider the strategies as black boxes and compare how they behave in a number of situations. This was the approach we chose for our so-called strategy vector.

Each agent to be analyzed was instantiated and fed fake game states, generated by iterating over number of visible cards, seating position, number of players who haven't folded yet, and investment into the pot (which also controlled pot size). For every combination of these, 10000 hands were generated, sorted into equally-sized card rank bins, and the agent's behavior was logged for every combination until sufficient samples were gathered for each bin.

The action space was divided into 53 bins, corresponding to folding, calling, checking, and raising in intervals of 4 chips (to a maximum of 200). Each of these bins contained the number of hands of a particular situation the agent chose to perform that action in.

The resulting \todo{check this number} 2880-dimensional list of situations with action frequencies in each was considered a characterization of that agent's playstyle, called that agent's strategy vector, and saved.

\todo{say something here about strat vector being useful both for quickly changing queries without recomputing all of the games, and also hopes for multidimensional clustering approaches that didn't work out all that well.}

Card ranks were computed differently based on the number of cards. For pre-flop, we used monte-carlo simulation until stability to generate a table of each possible pair, which let us rank them from best to worse, and divide these into deciles.

For flop, turns, and rivers, this is not feasible. Instead, we computed the card rank as evaluated by Treys \cite{Treys} directly, and compared that with a precomputed table of the number of hands with that number of cards that were better. This is not perfectly equal to the likelihood of a hand winning, but much easier to compute, so we used this approach as an approximation. \todo{consider giving an example, here}

\todo{IMAGE: card rank distributions: flop, turn, river}

\subsection{Aggression and Tightness}

Two common metrics for playstyle in poker are aggression and tightness \cite{PokerStrategy}.

Aggression compares the number of situations where a player increases the pot, vs just calling, and as such measures a player's proactivity. It can be defined as:
\begin{equation}
    Aggression = \frac{\# bets + \# raises}{\# calls}
\end{equation} where \# indicates the number of each betting action. Note that checks are not included in the calls.

Because this ratio can go all the way to infinity, it is somewhat difficult to plot, and the differences between agents are overstated at very small numbers of calls. To remedy this, we use a distortion of aggression in all our plots:
\begin{equation}
\begin{split}
Distorted\ Aggression &= \begin{cases}
\frac{Aggression}{Aggression + 1} &\mbox{if }Aggression\mbox{ is finite} \\
1 &\mbox{otherwise}
\end{cases}
\end{split}
\end{equation}
This compresses aggression to a nice $[0, 1]$ range, with $\frac{1}{2}$ being the natural midpoint of the same number of raises/bets and calls. \todo{footnote comparing this to odds -> probability}

Tightness is defined by how many hands a player folds. It is defined simply as:
\begin{equation}
    Tightness = \frac{\# folds}{\# calls + \# bets + \# raises}
\end{equation}
We decided to ignore checks, as most of our agents have a built-in behavior where they always check if they can and called or folded. This makes the number of checks mostly dependent on the situation numbers, and not on the agent behavior.

Both metrics were computed based off of the precomputed strategy vectors, as this both gave a consistent set of situations between agents (unlike game logs), and sufficient information to rapidly compute them.
\section{Agents}

\subsection{Baselines}
We used 2 kinds of non-learning agents during training, to give an initial population to train against.

The first was a random agent, who played randomly. Originally we tuned the likelihood of various actions to not have very short games most of the time, but as the poker competition used flat $\frac{1}{3}$ chance of each action, we did the same so our bots would adapt to the same baselines. It simply chooses from {fold, call/check, raise/bet} with equal probability, and in the case of a raise or bet chooses the amount randomly from what it still has available.

The other frequently used baseline was a call agent, that did nothing but call every hand in every situation.

\subsection{Q-Learning}
We decided to use OpenAI's SpinningUp (\cite{SpinningUp2018}) as a base implementation of Q-Learning, as it is well-tested, decently documented, and coherent with the guides and explanations on the SpinningUp website. This base code was modified to fit into our framework, though, as SpinningUp uses a very specific environment API and does training in a separate loop not all that practical for our purposes.

In doing so we also made few changes to the algorithm itself. The biggest change was dropping learning through the Bellman equation, through the simplification mentioned in equation \ref{Qlearn_nobellman}, instead training all Q-function outputs of a played trajectory $T$ towards the final reward $r$ with mean squared error loss:
\begin{equation}
    Loss = \sum_{(s, a) \in T} (Q(s, a) - r)^2
\end{equation}

Because the final reward of folding is always known ($r = -(current\ stake)$), we do not use Q-Learning to compute the outcome of folding, and the inverse of the current investment is used directly as the Q-function score of the folding action. This also allows us to avoid having to explore randomly folding in the middle of games, which dramatically increases skill performance and prevents the agent from playing overly cautious because it knows that it will throw a certain fraction of games.

Our input to all Q-Learning agents is the same 404-dimensional vector \todo{check this number} (as is also for SAC agents). It is composed of one-hot representation of the sorted hole cards, the sorted board cards (with an extra one-hot slot for "unknown"), which players have folded, which players put how much of their capital into the pot so far in the entire hand, which players put how much into the pot in this betting round, which player was the last to raise, which player is the current agent/where are we seated, which betting round we are currently in, and what is the card rank percentile of the current hand computed in the same way as described in section \ref{StrategyVectorComputation}.

We use two variants of action spaces in the later results, after some explorative experimentation with other variants: The Qlearn-8 agent with 8 different actions (\textit{call}, \textit{raise 4}, \textit{raise 8}, \textit{raise 16}, \textit{raise 32}, \textit{raise 64}, \textit{raise 100}, \textit{raise 200}), and a Qlearn-All agent with 201 different actions (\textit{call}, \textit{raise 1}, \textit{raise 2}, \dots, \textit{raise 199}, \textit{raise 200}).

Naively, Qlearn-All would perform around 20 times slower than the Qlearn-9 agent, as well as most others. For performance reasons, we added a small hack to Qlearn-All: Instead of computing the Q-function score of a state-action pair, it computes the Q-function score of a state for all actions at once, returning a 201-dimensional vector after one traversal. This theoretically "consumes" slightly more of the neural network, but in practice we found this to not matter a great deal in initial experiments.

Both variants used a noise $\epsilon$ of $0.1$ (10\% of actions were randomly sampled across all available actions except for folding) and a learning rate $0.001$, using an Adam optimizer. The networks themselves were a 4-layer feed-forward neural network, with each layer containing 256 neurons and a leaky ReLU activation function ($max(x, -0.001x)$), except for the last layer which used $tanh(x)$.

\subsection{Soft Actor-Critic}
Like Q-Learning, we based our implementation off of \cite{SpinningUp2018}, for much the same reasons. Unlike Q-Learning, we did not remove the bellman equation, as non-terminal states are relevant for total entropy, which is part of the Q-function score.

As such each SAC agent holds 5 neural networks: two Q-function networks, two smoothed target networks of the respective Q-functions, and the policy network.

We use the same input space for the SAC agents as the Q-Learning agents, but the action space is different: SAC agents return a 4-dimensional bounded floating-point vector, of which the first three dimensions are interpreted as desire to fold, call or raise (the highest is chosen as action), and the last as how much to raise by.

Our SAC agents all use a time discount factor of $\gamma = 1$ (no time discounting), Q-function network learning rate of $0.001$ (with Adam optimizer), policy network learning rate of $0.1$ (also with Adam), polyak averaging factor of $0.999$, and differ by entropy-weighing $\alpha$ value: SAC-High uses $\alpha = 0.01$, and SAC-Low uses $\alpha = 0.0005$. The neural networks themselves are identical to the ones used by the Q-Learning agents, with some added post-processing to the outputs of the policy network for the entropy calculation.

\section{Vectorized Poker Engine}
The competition itself in AIcrowd uses PyPokerEngine \cite{Pypokerengine} as an engine to simulate the actual poker games. This engine is built in a flexible way, but it is not optimized for speed, even after some modifications to use Treys \cite{Treys}. It also computes each hand one after another, making inference mini-batching almost impossible.

To train, we needed the ability to simulate millions of games, and ideally also to batch them so that the neural networks could be fed entire mini-batches of games at a time. This led us to develop our own poker simulation engine based primarily on Numpy \cite{Numpy} and Treys \cite{Treys}.

This engine computes batches of 10'000 hands per so-called round, all of them with the same agents in the same seating positions, but with different cards, bets, and victors. Almost every operation is performed as a numpy array operation, with one exception: hand score computation, because Treys does not support our batching method.

One major consequence of this is that games are computed "in parallel", and agents cannot form judgements based on the history of previous games. This excludes many potential agent architectures and opponent modeling strategies, but we found it an acceptable limitation for the performance gains.

To give an example of what this vectorizing looks like, here is some example code that computes the showdown winnings. Both \verb{hand_scores{ and \verb{total_winnings{ are 2-dimensional arrays of size \verb{(BATCH_SIZE, N_PLAYERS){, with \verb{hand_scores{ containing integer card ranks as computed by Treys and \verb{total_winnings{ the stack after each game. Side pots do not need to be considered, as all players have the same amount of money available. The somewhat exotic construction \verb{sorted_hands[:, 0][:, None]{ tiles the first column of \verb{sorted_hands{ to as many columns as is needed to match \verb{hand_scores{.
\begin{code}
ranks = np.argsort(hand_scores, axis=1)
sorted_hands = np.take_along_axis(hand_scores, indices=ranks, axis=1)
# Get everyone who has the best hand and among which pots will be split
participants = hand_scores == sorted_hands[:, 0][:, None]
# Get the number of times each pot will be split
n_splits_per_game = participants.sum(axis=1)
# Split and distribute the money
gains = pool / n_splits_per_game
total_winnings += participants * gains[:, None]
\end{code}

\todo{how much speed gain did this actually give us}