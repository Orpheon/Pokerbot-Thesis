\chapter{Implementation}

\section{Training Environment}
We set up a modular training environment to compare different agent populations. The different divisions can be used to observe how agents influence each other, which match up method produces the best results, and allows comparing agents that have not been trained against each other. \todo{fix wording}

\subsection{Agent Manager}
The agent manager is the single source of truth for all agents, and is responsible for instantiating agents. It tags every agent with a unique id so other components only need to store agent id. This removes redundancy and also makes it easy to change agent information without side effects.

The agent manager keeps all important information about an agent: type, paths to model files, whether the agent is trainable or not, origin division and its name.
\todo{explain more}

\subsection{Leaderboard}
A leaderboard ranks agents from best to worst according to one of the metrics. There are several different types of leaderboards using different metrics (e.g. TrueSkill, winnings, etc.). When a match in a division concludes, all leaderboards attached to the division are updated with the results. Further detail on the different metrics in section \ref{PerformanceMetricsSection}.

\todo{matchup, match-up, or match up?}

\subsection{Divisions}
A division is responsible for generating match ups between agents, running games, and updating leaderboards with the results. There are several types of divisions with different match up rules.

Divisions differentiate between two types of agents: teachers and students. A student is trainable, using one of the algorithms discussed in section \ref{ReinforcementLearningChapter} and adapts its behavior during play. A teacher does not change its behavior and is either a baseline with hard-coded behavior or a clone of a student with fixed weights. Divisions usually have their own set of agents and do not influence outside agents. However, there are some exceptions, as will be explained in sections \ref{PermEvalSimilarDiv} and \ref{PermEvaldirectDiv}.

\subsubsection{Random Division}
The random division requires at least one student and one teacher, and keeps a list of its own agents. Matchups are generated by randomly selecting one student and five teachers. These teachers do not have to be distinct; it is possible that the same agent is chosen five times.

\subsubsection{Climbing Division}
The climbing division also requires at least one student and one teacher, and keeps a list of its own agents. Unlike the random division, this division requires a TrueSkill-based leaderboard to function. The goal of this division is to match students more frequently against better teachers.

This division selects teachers weighted by their TrueSkill rating. Let $\mu_i$ denote the TrueSkill rating of the $i$'th agent, then that agent's probability $p(Selected)$ among the $n$ other agents is:
\begin{equation}
  p(Selected) = \frac{{\mu_i}^2}{\sum_{j=0}^{n} {\mu_j}^2}
\end{equation}
This is repeated five times, making it likely that the best agent is chosen multiple times.

This has two problems: first, a new agent is assumed to have an average rating, but is likely one of the best agents. Because of this, the agent might only rarely play and therefore take a long time to converge to its actual TrueSkill rating. Second, with a large number of agents there is an inequality between the number of games played per agent which also causes the TrueSkill rating of rarely appearing agents to be unreliable.

To counteract these problems, there is a fixed probability that instead of regular matchups with one student and five teachers, a matchup is generated with six teachers. This has the sole purpose of improving the reliability of the TrueSkill ratings and therefore improving regular matchups. For this six teacher matchup a random agent is selected and five other teachers with similar ranking are chosen as opponents, if enough agents are available no agent is chosen multiple times as to maximise the number of agent rankings being updated.

\subsubsection{Permanent Evaluation Similar Division}\label{PermEvalSimilarDiv}
Short "PermEvalSimilar", this division is intended to evaluate all agents across all divisions, not agents inside itself. The goal is to evaluate agents between divisions to not only test how agents react to previously unknown opponents but also to evaluate which divisions produce better agents.

This division does not have an internal agent list, and uses the agent manager directly. It looks at all available agents, except for students to not influence the learning process. It selects one agent at random and chooses five other agents with similar rankings, all unique if enough are available.

\subsubsection{Permanent Evaluation Direct Division}\label{PermEvaldirectDiv}
Short "PermaEvalDirect", this division is special, as it compares only two agents against each other. It selects two random agents from the entire pool and creates three instances of each agent. The two agents must be different and the division tracks previously run matchups and prioritizes those that were not run before.

This division is specifically designed to compute the winnings metric (section \ref{Winnings}).

\subsection{Division Manager}
The division manager works similarly to the agent manager in that it handles all divisions with their configurations and tags every division with a unique id. This is used to simplify the running of games, as user can simply add divisions with the desired configuration and agent constellation and then specify which divisions should be run.

\section{Performance Metrics}\label{PerformanceMetricsSection}

\subsection{TrueSkill}
We did not reimplement TrueSkill, instead using an existing python implementation \cite{TrueSkill_code}. We used a $\beta$ of $\frac{25}{6}$ and a starting distribution $\mathcal{N}(\mu=100, \sigma=30)$ for new agents.

\subsection{Winnings}
\label{Winnings}
The winnings metric compares agents in a direct confrontation against each other by matching up both agents with three copies each. For both the average winning is calculated and added to a matrix of all agents.

Seats are randomized to minimize the advantage gained by a better seat. Ideally all seating combinations would be run for an equal number of hands, but this was not implemented because of increased complexity and computational time needed.

To improve runtime, matchups that have not been played are first in line for the next evaluation. Because of the quadratic growth of matchups needed this metric is not a good choice for a large number of agents.

To generate a leaderboard from the winnings matrix, the following measures are used:
\begin{itemize}
    \item mean of winnings
    \item median of winnings, which has the advantage of not being distorted by different performance against terrible agents (e.g. baseline agents)
    \item percentiles (e.g. 20pctl) of winnings, which has the advantage of showing how the agent performs against its worst matchups, giving a measure of robustness
\end{itemize}

\subsection{Kendall's Tau}

We use Kendall's Tau \cite{KendallTau} in section \ref{Results} as a measure of distance between rankings. This allows us to describe the similarity between rankings of different measures or between different divisions containing the same agents.

\todo{short explanation of what kendall's tau actually does}

We did not implement the measure ourselves, instead using SciPy's implementation \cite{Scipy} with default parameters.

\section{Strategy Analysis}

We use several metrics to try to figure out how different agents behave, and how they differ in their behavior. Some of these are primarily for debugging, others for insight-generation.

\subsection{Cashflow}

Some agents play worse than others in all situations. Others might usually win, but lose hard against certain exploiters. Yet others prefer to "avoid losing" and make small gains but never really exploit others.

To distinguish which agents were exploiting which how much, we measure cashflow; how much money was each agent giving over to each agent on average.

Cashflow is computed individually per round, though conceptually it would not be impossible to compute it on average throughout. However, it requires extensive logs containing the individual results of every single hand, to track who wins what, which slows training considerably.

While useful for debugging specific early agents, cashflow was both too slow and requires too many games until convergence to be a useful metric in larger runs, so we did not use it in section \ref{Results}.

\subsection{Strategy Vector}
\label{StrategyVectorComputation}
To compare two different player strategies, one concept is to consider strategies as black boxes and compare how they behave in a number of situations. This was the approach we chose for our so-called strategy vector.

Each agent to be analyzed is instantiated and fed fake game states, generated by iterating over the number of visible cards, seating position, number of players who haven't folded yet, and investment into the pot (which also controls pot size). For every combination of these variables, 10'000 hands were generated, sorted into equally-sized card rank bins, and the agent's behavior was logged for every combination until sufficient samples were gathered for each bin.

The action space is divided into 53 bins, corresponding to folding, calling, checking, and raising in intervals of 4 (to a maximum of 200). Each of these bins containes the number of hands the agent performs that action in that situation.

The resulting 4'320-dimensional array of situations with action frequencies (in 53 bins) is considered a characterization of that agent's play-style, called that agent's strategy vector, and saved to disk (a 228'960-dimensional array in total, approximately 1.8 MB per agent).

Computing this vector takes a few minutes per agent, and represents the bulk of computation required for strategy plotting, allowing quick generation of different plots

Card ranks are calculated differently based on the number of cards. For pre-flop, we use monte-carlo simulation until convergence to generate a table of the victory likelihood of each possible pair, which lets us rank them from best to worse, and divide these into deciles.

For flop, turns, and rivers, this is not feasible. Instead, we computed the card rank as evaluated by Treys \cite{Treys} directly, and compared that with a precomputed table of the number of possible hands that have higher rank. This is not perfectly equal to the likelihood of a hand winning, but much easier to compute, so we used this approach as an approximation.

\subsection{Aggression and Tightness}

Two common metrics for playstyle in poker are aggression and tightness \cite{PokerStrategy}.

Aggression compares the number of situations in which a player increases the pot vs just calling, and as such measures a player's proactivity. It can be defined as:
\begin{equation}
    Aggression \coloneqq \frac{\# bets + \# raises}{\# calls}
\end{equation} where \# indicates the number of each betting action. Note that checks are not included in the calls.

Because this ratio can go all the way to infinity, it is difficult to plot, and the differences between agents are overstated at small numbers of calls. To remedy this, we use a distortion of aggression in all our plots:
\begin{equation}
\begin{split}
Distorted\ Aggression &\coloneqq \begin{cases}
\frac{Aggression}{Aggression + 1} &\mbox{if }Aggression\mbox{ is finite} \\
1 &\mbox{otherwise}
\end{cases}
\end{split}
\end{equation}
This compresses aggression to a $[0, 1]$ range, with $\frac{1}{2}$ being the natural midpoint of the same number of raises/bets and calls\footnote{This is similar to converting odds ratio to probabilities, which uses the same formula and also compresses into a $[0, 1]$ range without losing too much interpretability.}.

Tightness is defined by how many hands a player folds:
\begin{equation}
    Tightness \coloneqq \frac{\# folds}{\# calls + \# bets + \# raises}
\end{equation}
We decided to ignore checks, as most of our agents have a built-in behavior where they always check if they can and want to call or fold. This makes the number of checks mostly dependent on the situation distribution, and not on the agent behavior.

Both metrics are computed based off of the precomputed strategy vectors, as this both gives a consistent set of situations between agents (unlike game logs), and sufficient information to rapidly compute them.
\section{Agents}

\subsection{Baselines}
We used 2 kinds of baseline agents during training, to give an initial population to train against.

The first was a random agent, who played randomly. Originally we tuned the likelihood of actions to prolong game length, but as the competition baseline used a flat $\frac{1}{3}$ chance for each action, we did the same so our bots would adapt to the same baselines. It chooses from \{fold, call/check, raise/bet\} with equal probability, and in the case of a raise or bet chooses the amount randomly from what it still has available.

The other frequently used baseline was a call agent, that did nothing but call every hand in every situation.

\subsection{Q-Learning}
We used OpenAI's SpinningUp (\cite{SpinningUp2018}) as a base implementation of Q-Learning, as it is well-tested, well documented, and coherent with the guides and explanations on the SpinningUp website. This base code was modified to fit into our framework as SpinningUp uses a very specific environment API and does training in a separate loop impractical for our purposes.

In doing so we also made changes to the algorithm. The biggest change was dropping the Bellman equation through the simplification mentioned in equation \ref{Qlearn_nobellman}, instead training all Q-function outputs of a played trajectory $T$ towards the final reward $r$ with mean squared error loss:
\begin{equation}
    Loss \coloneqq \sum_{(s, a) \in T} (Q(s, a) - r)^2
\end{equation}

Because the final reward of folding is always known ($r = -(current\ stake)$), we do not use Q-Learning to compute the outcome of folding, and the inverse of the current investment is used directly as the Q-function score of the folding action. This allows us to avoid having to explore randomly folding in the middle of games, which dramatically increases skill and prevents the agent from playing too cautiously because it knows that it will throw a fraction of games.

Our input to all Q-Learning agents is the same 404-dimensional vector (as is also for SAC agents). It is composed of one-hot representation of the sorted hole cards, the sorted community cards (with an extra one-hot slot for "hidden"), which players have folded, which players put how much of their capital into the pot so far in the entire hand, which players put how much into the pot in this betting round, which player was the last to raise, which player is the current agent/where it is seated, which betting round it is in, and what is the card rank percentile of the current hand computed in the same way as described in section \ref{StrategyVectorComputation}.

We use two variants of action spaces in the results section, after some explorative experimentation with other variants: The Qlearn-8 agent with 8 different actions (call, raise 4, raise 8, raise 16, raise 32, raise 64, raise 100, raise 200), and a Qlearn-All agent with 201 different actions (call, raise 1, raise 2, \dots, raise 199, raise 200).

Naively implemented, Qlearn-All would perform around 20 times slower than the Qlearn-9 agent. For performance reasons, we added a hack to Qlearn-All: Instead of computing the Q-function score of a state-action pair, it computes the Q-function score of a state for all actions at once, returning a 201-dimensional vector after one traversal. This theoretically "consumes" more of the neural network, but in practice we found this to not matter in initial experiments.

Both variants used a noise $\epsilon$ of $0.1$ (10\% of actions were randomly sampled across all available actions except for folding) and a learning rate of $0.001$, using an Adam optimizer. The network is a 4-layer feed-forward neural network, with each layer containing 256 neurons and a leaky ReLU activation function ($max(x, -0.001x)$), except for the last layer which uses $tanh(x)$.

\subsection{Soft Actor-Critic}
Like Q-Learning, we based our implementation off of \cite{SpinningUp2018}, for the same reasons. Unlike Q-Learning, we did not remove the Bellman equation, as non-terminal states are relevant for total entropy, which in SAC is part of the Q-function score.

Each SAC agent holds 5 neural networks: two Q-function networks, two polyak-averaged target networks of the respective Q-functions, and the policy network.

We use the same input space for SAC agents as Q-Learning agents, but the action space is different: SAC agents return a 4-dimensional bounded floating-point vector, of which the first three dimensions are interpreted as desire to fold, call or raise (the highest is chosen as action), and the last as how much to raise by.

Our SAC agents use a time discount factor $\gamma = 1$ (no time discounting), Q-function network learning rate of $0.001$ (with Adam optimizer), policy network learning rate of $0.1$ (also with Adam), polyak averaging factor of $0.999$, and differ by temperature: SAC-High uses $\alpha = 0.01$, and SAC-Low uses $\alpha = 0.0005$. The neural networks are identical to the one used by Q-Learning agents, with some added post-processing to the outputs of the policy network for action sampling and entropy calculation.

\section{Vectorized Poker Engine}
The competition itself in AIcrowd uses PyPokerEngine \cite{Pypokerengine} as engine to simulate the poker games. This engine is built in a flexible way, but it is not optimized for speed, even after some modifications to use Treys \cite{Treys} for card rank calculation. It also computes each hand one after another, making inference mini-batching almost impossible.

To train, we need the ability to simulate millions of games, and ideally also to batch them so that the neural networks can be fed entire mini-batches of games at a time. This led us to develop our own poker simulation engine based primarily on Numpy \cite{Numpy} and Treys.

This engine computes batches of 10'000 hands per so-called round, all of them with the same agents in the same seating positions, but with different cards, bets, and victors. Almost every operation is performed as a numpy array operation, except hand score computation, as Treys does not support our batching method.

One consequence of this is that games are computed "in parallel", and agents cannot form judgements based on the history of previous games. This excludes many potential agent architectures and opponent modeling strategies, but we found it an acceptable limitation for the performance gains, and the competition rules make relying on history unreliable.

To give an example of what this vectorizing looks like, here is some example code that computes showdown winnings. Both \verb{hand_scores{ and \verb{total_winnings{ are 2-dimensional arrays of size \verb{(BATCH_SIZE, N_PLAYERS){, with \verb{hand_scores{ containing integer card ranks as computed by Treys and \verb{total_winnings{ the stack after each game. Side pots do not need to be considered, as all players have the same amount of money available. The somewhat exotic construction \verb{sorted_hands[:, 0][:, None]{ tiles the first column of \verb{sorted_hands{ to as many columns as is needed to match \verb{hand_scores{.
\begin{code}
ranks = np.argsort(hand_scores, axis=1)
sorted_hands = np.take_along_axis(hand_scores, indices=ranks, axis=1)
# Get everyone who has the best hand and among which pots will be split
participants = hand_scores == sorted_hands[:, 0][:, None]
# Get the number of times each pot will be split
n_splits_per_game = participants.sum(axis=1)
# Split and distribute the money
gains = pool / n_splits_per_game
total_winnings += participants * gains[:, None]
\end{code}

\todo{how much speed gain did this actually give us}