\chapter{Conclusions}

\section{Populations}
Contrary to our initial expectation, we find that different populations do not lead to visible differences in strategy coverage, and matter far less than agent structure (Figure \ref{AggTightDivision}, Figure \ref{AggTightAgentType}).

Matchup generation does matter, but only marginally, and is a tradeoff; while climbing is better for mean gain, random creates more robust agents. This makes sense as agents in random matchups play against all agents equally, whereas climbing agents spend more time practicing against better agents.
We also find that matchup generation (climbing vs random) does not influence training strongly (Figure \ref{MatchupDistribution}).

\section{Metrics}
TrueSkill does not converge but gets close (Figure \ref{TrueSkillCompare}). Its consistency between runs depends on the agent type (Figure \ref{TrueSkillCompare2}), being quite high even after few runs for Qlearn agents, and terrible for SAC agents even after 5'000 rounds.

The different metrics do not agree (Table \ref{TableTopAgentsMetrics}), though some agents perform well in all of them. They favor different strategies (Figure \ref{AggTightRank}), though not as much as expected.

\section{Agents}
\label{ConclusionAgents}

SAC agents perform poorly (Figures \ref{AgentTypeDistribution} and \ref{AgentGenerations}), despite significant effort and experimentation in the early stages of this project. They also play inconsistently (Figures \ref{TrueSkillCompare2} and \ref{DivisionConsistency}).

The action space of Q-Learning agents influences their behavior heavily, with Qlearn-All playing more aggressively than any other architecture (Figure \ref{AggTightAgentType}).
We believe this is because of how exploration is implemented; every action has a $\epsilon = 10\%$ chance of being replaced with a randomly selected non-folding action from the action space. The action space for Qlearn-All is composed of one call action and 199 raise actions, whereas Qlearn-8 uses one call action and only 7 raise actions. This means that Qlearn-All explores aggressive actions more than Qlearn-8, and more than other agents like SAC. A further experiment would be to weigh this exploration likelihood by action type, such that calling is explored as much as raising in total.

\section{Future Work}

Testing how Qlearn-All develops given a more balanced exploration between calling and raising is a simple change and could hold much insight.

Our experiments have only tested 2'000 rounds, with clones every 200 rounds. It is difficult to see whether performance is capped out from the generation plots (Figure \ref{AgentGenerations}), but we suspect not and it would be interesting to see results of a much longer training period with an order of magnitude more agents, with fewer different leagues.

Another open possibility is using the strategy vector to filter a subset of agents that behave in a certain way and overfitting a new agent against them. This might create dedicated exploiter agents, whose behavior could give insight into the game itself.