\chapter{Conclusions}

\section{Populations}
Contrary to our initial expectation, we find that different populations do not lead to visible differences in strategy coverage, and matter far less than agent structure \todo{reference relevant section in results}.

We also find that matchup generation (climbing vs random) does not influence training strongly after sufficient rounds.

\section{Metrics}
TrueSkill does not converge well, though it remains close \todo{reference}. It is also less insightful than other metrics, as it does not show whether an agent is robust or only good in certain matchups. 
Additionally, the matchmaking method chosen has a huge influence on TrueSkill rankings \todo{reference}. This leads us to prefer using winnings to judge an agent's performance.

Comparison against baselines does not, as expected, correlate almost at all with performance against other agents. \todo{reference}

\section{Agents}
SAC agents do not perform well \todo{reference}, despite significant effort and experimentation in the early stages of this project. It is possible this is because applying a continuous action space algorithm on poker was fundamentally a wrong approach, or because the entropy bonus was not handled correctly.

The action space of the Q-Learning agents influences their behavior heavily, with the Qlearn-All agents playing more aggressively than any other architecture.
One explanation for this is how exploration is handled; every action has a $\epsilon = 10\%$ chance of being replaced with a randomly selected non-folding action from the action space. The action space for qAll is composed of one call action and 199 raise actions, whereas qln5 uses one call action and only 7 raise actions. This means that qAll explores aggressive actions more than qln5, and also more than other agents like SAC. A further experiment would be to weigh this exploration likelihood by action type, such that calling is explored as much as raising in total. \todo{Standardize agent naming} \todo{More text on future work}